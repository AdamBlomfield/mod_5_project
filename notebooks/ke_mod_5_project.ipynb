{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Not Delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hidecode"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables\n"
     ]
    }
   ],
   "source": [
    "# DO NOT REMOVE THESE\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT REMOVE This\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT REMOVE\n",
    "## import local src module -\n",
    "## src in this project will contain all your local code\n",
    "## clean_data.py, model.py, visualize.py, custom.py\n",
    "\n",
    "#import src.base with alias to avoid namespace clashes\n",
    "\n",
    "from src import base as fis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Module Imported\n",
      "\n",
      "Testing local imports\n",
      "In clean_data\n",
      "In Model\n",
      "In Visualize\n",
      "In custom module\n"
     ]
    }
   ],
   "source": [
    "fis.test_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-28T12:36:34.194734Z",
     "start_time": "2019-06-28T12:36:33.750982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Preparation\n",
    "    # Train:Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "    # Normalize\n",
    "from sklearn.preprocessing import normalize\n",
    "    # Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "    # GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "    # PCA\n",
    "from sklearn.decomposition import PCA\n",
    "    \n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.tree import DecisionTreeClassifier  # Decision Tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier  #Bagging & Random Forest\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "    # Visuals for Random Forest\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "# Logistic Regression\n",
    "    # SK-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "    # Statsmodel\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = None  #put as a string\n",
    "df = pd.read_csv(csv_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam's Code\n",
    "\n",
    "# overall look\n",
    "df.head()\n",
    "df.shape\n",
    "df.columns\n",
    "\n",
    "# values\n",
    "df.info()\n",
    "df['col1'] = df['col1'].astype(float) #change dtype\n",
    "df.isna().sum().sort_values(ascending=False) #sorted na values\n",
    "df.describe()\n",
    "for column in df.columns:\n",
    "    print(df[column].value_counts())\n",
    "  # replace weird values\n",
    "df['col1'] = df['col1'].replace('?',np.nan)\n",
    "  # get dummies\n",
    "pd.get_dummies(df['col1'],prefix='col1',prefix_sep='_', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Get dummies for the columns we're interested if\n",
    "x_feats = ['col1', 'col2', 'col3']\n",
    "X = pd.get_dummies(df[x_feats])\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our cleaned dataframe to cleaned_dataframe.csv\n",
    "df.to_csv('../data/processed/cleaned_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('../data/processed/cleaned_dataframe.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model as a png file\n",
    "png_filename = NONE.png\n",
    "plt.savefig('../reports/figures/{}'.format(png_filename), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction through PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam's Code\n",
    "pca = PCA(n_components=2)  #ncomponents are your number of variables that your trying to keep, they're not your target variables\n",
    "pca.fit(X)\n",
    "\n",
    "print(pca.components_)\n",
    "# This tells us the variance explained by each line (keep the greater one)\n",
    "print(pca.explained_variance_)  #nb that this is the percentage explained of total, they wont add up to 1\n",
    "\n",
    "# Show the vectors on a scatter plot of the variables\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "    # plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');\n",
    "\n",
    "# Using PCA to reduce dimensionality\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "    # visual to show the dimensionality reduction\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], alpha=0.8)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n_bayes = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For continous\n",
    "df_g_bayes = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam's Code\n",
    "\n",
    "# Scale first\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict and show accuracy\n",
    "    # make class predictions for the testing set\n",
    "y_pred_class = knn.predict(X_test)\n",
    "    # calculate accuracy\n",
    "from sklearn import metrics\n",
    "print('Accuracy:' + str(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "print('F1: ' + str(metrics.f1_score(y_test, y_pred_class)))\n",
    "\n",
    "# F1 score by K values + visualization for k Scores\n",
    "import matplotlib.pyplot as plt\n",
    "def f1_by_k_value(X_train, y_train, X_test, y_test, krange_min=1,krange_max=50):\n",
    "    '''calculate the F1 score by k-value and plot a visualization'''\n",
    "    # Build list of k-scores\n",
    "    k_range = list(range(krange_min, krange_max))\n",
    "    k_scores = []\n",
    "    for k in k_range:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_predict = knn.predict(X_test)\n",
    "        score = metrics.f1_score(y_test, y_predict, average='weighted')\n",
    "        k_scores.append( score)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(k_range, k_scores, color='red', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='blue', markersize=10)\n",
    "    plt.title('F1 score by K Value')\n",
    "    plt.xlabel('K Value')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.show()\n",
    "\n",
    "# Get the accuracy\n",
    "knn = KNN()\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)\n",
    "print(\"Testing Accuracy: {}%\".format(round(accuracy_score(y_test, preds),4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_reg = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam's code for SKlearn\n",
    "\n",
    "# Get dummies for the columns we're interested if\n",
    "x_feats = ['col1', 'col2', 'col3']\n",
    "X = pd.get_dummies(df[x_feats])\n",
    "y = df.target\n",
    "    # Fill null values\n",
    "X = X.fillna(value=0)\n",
    "    # Normalize data\n",
    "for col in X.columns:\n",
    "    X[col] = (X[col]-min(X[col]))/ (max(X[col]) - min(X[col])) #We subtract the minimum and divide by the range forcing a scale of 0 to 1 for each feature\n",
    "    # Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "    # Fit model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12)\n",
    "model_log = logreg.fit(X_train, y_train)\n",
    "model_log\n",
    "    # Predict\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "        # Classifier correctness on training data\n",
    "residuals = np.abs(y_train - y_hat_train)\n",
    "print(pd.Series(residuals).value_counts())\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "correct = pd.Series(residuals).value_counts(normalize=True)[0]\n",
    "print('\\nOur model is {}% accurate for our training data'.format(round(correct*100,2)))\n",
    "        # Classifier correctness on testing data\n",
    "residuals = np.abs(y_test - y_hat_test)\n",
    "print(pd.Series(residuals).value_counts(),'\\n')\n",
    "print(pd.Series(residuals).value_counts(normalize=True))\n",
    "correct = pd.Series(residuals).value_counts(normalize=True)[0]\n",
    "print('\\nOur model is {}% accurate for our test data'.format(round(correct*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam's Code Statsmodel version\n",
    "\n",
    "# Define X and Y\n",
    "y, X = dmatrices('target ~ col1 + C(categorical_col) + col3' ,\n",
    "                  df, return_type = \"dataframe\")\n",
    "\n",
    "# Fit the 1st model\n",
    "logit_model = sm.Logit(y,X)\n",
    "result = logit_model.fit()\n",
    "    # See results of 1st model\n",
    "model1_summary = result.summary()\n",
    "model1_summary\n",
    "\n",
    "# Drop rubbish pvalues\n",
    "alpha = 0.05\n",
    "variables_to_keep = []\n",
    "for X_variable, pvalue in list(zip(X,result.pvalues)):\n",
    "    if pvalue <= alpha:\n",
    "        variables_to_keep.append(X_variable)\n",
    "variables_to_keep  # leaves behind the variables that have p value below the alpha\n",
    "X = X.drop(columns=variables_to_keep)  # take out the variables\n",
    "\n",
    "# Fit the 2nd model\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "    # see results of 2nd model\n",
    "model2_summary = result.summary()\n",
    "model2_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_random = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam's Code\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=10)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test,y_pred) * 100\n",
    "print(\"Accuracy is : {}%\".format(round(acc, 2)))\n",
    "    # Check the AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUC is :{0}\".format(round(roc_auc,2)))\n",
    "    # Create and print a confusion matrix\n",
    "print('\\nConfusion Matrix')\n",
    "print('----------------')\n",
    "pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "# Train a DT classifier\n",
    "classifier2 = DecisionTreeClassifier(random_state=10, criterion='entropy')\n",
    "classifier2.fit(X_train, y_train)\n",
    "    # Make predictions for test data\n",
    "y_pred = classifier2.predict(X_test)\n",
    "    # Calculate Accuracy\n",
    "acc = accuracy_score(y_test,y_pred) * 100\n",
    "print(\"Accuracy is :{0}\".format(acc))\n",
    "    # Check the AUC for predictions\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print(\"\\nAUC is :{0}\".format(round(roc_auc,2)))\n",
    "    # Create and print a confusion matrix\n",
    "print('\\nConfusion Matrix')\n",
    "print('----------------')\n",
    "print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "\n",
    "\n",
    "# Function for plotting feature importance\n",
    "classifier2.feature_importances_ # How much does this feature reduce entropy\n",
    "    # Function to show importance of each feature\n",
    "def plot_feature_importances(model):\n",
    "    '''plots the importance of each feature.  Useful for something like KNN so you can tell which features are useful '''\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "plot_feature_importances(classifier2)\n",
    "    # confusion matrix print\n",
    "pred = classifier2.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "# Random Forest in code\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "forest.fit(X_train, y_train)\n",
    "forest.score(X_train, y_train) # Accuracy of training data\n",
    "forest.score(X_test, y_test) # Accuracy of test data\n",
    "plot_feature_importances(forest) # plot using function above\n",
    "\n",
    "# Fine Tune the model\n",
    "forest_2 = RandomForestClassifier(n_estimators = 10, max_features= 2, max_depth= 2)\n",
    "forest_2.fit(X_train, y_train)\n",
    "forest_2.score(X_train, y_train) # Accuracy of training data\n",
    "forest_2.score(X_test, y_test)  # Accuracy of test data\n",
    "\n",
    "# Use GridsearchCV to find the best Number\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_estimators': [30, 100, 300], 'min_samples_split': [2, 4, 6], 'min_samples_leaf': [2, 4, 6]}\n",
    "gs = GridSearchCV(forest, param_grid, cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Mean accuracy on the given test data and labels:', gs.score(X_test, y_test))\n",
    "print('The best parameters are:', gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagged Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv to df\n",
    "df = pd.read_csv('filename.csv')\n",
    "    # Train Test Split\n",
    "X = df.drop('y', axis=1)\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "# SVC\n",
    "from sklearn.svm import SVC\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(X_train, y_train)\n",
    "    # Print out results\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix , accuracy_score\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(\"The accuracy score is {}\".format(round(accuracy_score(y_test, y_pred), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
